{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a43b7f-bcd7-4ad5-a476-13c1bcc0f9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ef6713-2435-4652-9b87-621b8816a794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fe408d-1836-4c9a-b109-550e0ceb4a0d",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c88ab5ce-1c41-467b-88e2-45dd6b00e7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pizza_steak_sushi directory does not exists, creating one...\n",
      "downloading, dataset...\n",
      "dataset downloaded\n",
      "unzipping dataset...\n",
      "dataset unzipped in data/pizza_steak_sushi\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path('data/')\n",
    "image_path = data_path / 'pizza_steak_sushi'\n",
    "\n",
    "if image_path.is_dir():\n",
    "    print(f'{image_path} directory already exists')\n",
    "else:\n",
    "    print(f'{image_path} directory does not exists, creating one...')\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(data_path / 'pizza_steak_sushi.zip', 'wb') as f:\n",
    "        print('downloading, dataset...')\n",
    "        req = requests.get('https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip')\n",
    "        f.write(req.content)\n",
    "    print('dataset downloaded')\n",
    "        \n",
    "    with zipfile.ZipFile(data_path / 'pizza_steak_sushi.zip', 'r') as zip_ref:\n",
    "        print('unzipping dataset...')\n",
    "        zip_ref.extractall(image_path)\n",
    "    print(f'dataset unzipped in {image_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ebfab4c-6120-42d3-b2c0-0938eea02396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/pizza_steak_sushi/train'),\n",
       " PosixPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "train_dir = image_path / 'train'\n",
    "test_dir = image_path / 'test'\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe22981f-60e8-41f1-83b2-2c98043f0165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 225\n",
      "    Root location: data/pizza_steak_sushi/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "           )\n",
      "Test data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 75\n",
      "    Root location: data/pizza_steak_sushi/test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Create simple transform\n",
    "data_transform = transforms.Compose([ \n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Use ImageFolder to create dataset(s)\n",
    "train_data = datasets.ImageFolder(root=train_dir, # target folder of images\n",
    "                                  transform=data_transform, # transforms to perform on data (images)\n",
    "                                  target_transform=None) # transforms to perform on labels (if necessary)\n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir, \n",
    "                                 transform=data_transform)\n",
    "\n",
    "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f89e71f-acd0-4d79-bf92-430db81e3ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['pizza', 'steak', 'sushi'], 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.classes\n",
    "output_shape = len(class_names)\n",
    "\n",
    "class_names, output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d557c-d6f1-46d2-b7c6-913675932dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7fdea971ff10>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7fdea971ea10>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=os.cpu_count(),\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             num_workers=os.cpu_count(),\n",
    "                             batch_size=BATCH_SIZE)\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b93b1c2-213b-48a0-ac8b-60c35c455c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "  \"\"\"Creates the TinyVGG architecture.\n",
    "\n",
    "  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
    "  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n",
    "  \n",
    "  Args:\n",
    "    input_shape: An integer indicating number of input channels.\n",
    "    hidden_units: An integer indicating number of hidden units between layers.\n",
    "    output_shape: An integer indicating number of output units.\n",
    "  \"\"\"\n",
    "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "      super().__init__()\n",
    "      self.conv_block_1 = nn.Sequential(\n",
    "          nn.Conv2d(in_channels=input_shape, \n",
    "                    out_channels=hidden_units, \n",
    "                    kernel_size=3, # how big is the square that's going over the image?\n",
    "                    stride=1, # default\n",
    "                    padding=0), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=hidden_units, \n",
    "                    out_channels=hidden_units,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2,\n",
    "                        stride=2) # default stride value is same as kernel_size\n",
    "      )\n",
    "      self.conv_block_2 = nn.Sequential(\n",
    "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(2)\n",
    "      )\n",
    "      self.classifier = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          # Where did this in_features shape come from? \n",
    "          # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "          nn.Linear(in_features=hidden_units*13*13,\n",
    "                    out_features=output_shape)\n",
    "      )\n",
    "    \n",
    "  def forward(self, x: torch.Tensor):\n",
    "      x = self.conv_block_1(x)\n",
    "      x = self.conv_block_2(x)\n",
    "      x = self.classifier(x)\n",
    "      return x\n",
    "      # return self.classifier(self.block_2(self.block_1(x))) # <- leverage the benefits of operator fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f07b6394-d714-4846-a701-5bc8ae08bd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyVGG(\n",
       "  (conv_block_1): Sequential(\n",
       "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1690, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate an instance of the model\n",
    "torch.manual_seed(42)\n",
    "model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n",
    "                  hidden_units=10, \n",
    "                  output_shape=len(train_data.classes)).to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a901c3fa-a410-4e07-a557-000302270e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "  \"\"\"Trains a PyTorch model for a single epoch.\n",
    "\n",
    "  Turns a target PyTorch model to training mode and then\n",
    "  runs through all of the required training steps (forward\n",
    "  pass, loss calculation, optimizer step).\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model to be trained.\n",
    "    dataloader: A DataLoader instance for the model to be trained on.\n",
    "    loss_fn: A PyTorch loss function to minimize.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "  Returns:\n",
    "    A tuple of training loss and training accuracy metrics.\n",
    "    In the form (train_loss, train_accuracy). For example:\n",
    "    \n",
    "    (0.1112, 0.8743)\n",
    "  \"\"\"\n",
    "  # Put model in train mode\n",
    "  model.train()\n",
    "  \n",
    "  # Setup train loss and train accuracy values\n",
    "  train_loss, train_acc = 0, 0\n",
    "  \n",
    "  # Loop through data loader data batches\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "      # Send data to target device\n",
    "      X, y = X.to(device), y.to(device)\n",
    "\n",
    "      # 1. Forward pass\n",
    "      y_pred = model(X)\n",
    "\n",
    "      # 2. Calculate  and accumulate loss\n",
    "      loss = loss_fn(y_pred, y)\n",
    "      train_loss += loss.item() \n",
    "\n",
    "      # 3. Optimizer zero grad\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # 4. Loss backward\n",
    "      loss.backward()\n",
    "\n",
    "      # 5. Optimizer step\n",
    "      optimizer.step()\n",
    "\n",
    "      # Calculate and accumulate accuracy metric across all batches\n",
    "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "  # Adjust metrics to get average loss and accuracy per batch \n",
    "  train_loss = train_loss / len(dataloader)\n",
    "  train_acc = train_acc / len(dataloader)\n",
    "  return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a1aa2af-242b-4076-b0df-9f925fb95489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "  \"\"\"Tests a PyTorch model for a single epoch.\n",
    "\n",
    "  Turns a target PyTorch model to \"eval\" mode and then performs\n",
    "  a forward pass on a testing dataset.\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model to be tested.\n",
    "    dataloader: A DataLoader instance for the model to be tested on.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "  Returns:\n",
    "    A tuple of testing loss and testing accuracy metrics.\n",
    "    In the form (test_loss, test_accuracy). For example:\n",
    "    \n",
    "    (0.0223, 0.8985)\n",
    "  \"\"\"\n",
    "  # Put model in eval mode\n",
    "  model.eval() \n",
    "  \n",
    "  # Setup test loss and test accuracy values\n",
    "  test_loss, test_acc = 0, 0\n",
    "  \n",
    "  # Turn on inference context manager\n",
    "  with torch.inference_mode():\n",
    "      # Loop through DataLoader batches\n",
    "      for batch, (X, y) in enumerate(dataloader):\n",
    "          # Send data to target device\n",
    "          X, y = X.to(device), y.to(device)\n",
    "  \n",
    "          # 1. Forward pass\n",
    "          test_pred_logits = model(X)\n",
    "\n",
    "          # 2. Calculate and accumulate loss\n",
    "          loss = loss_fn(test_pred_logits, y)\n",
    "          test_loss += loss.item()\n",
    "          \n",
    "          # Calculate and accumulate accuracy\n",
    "          test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "          \n",
    "  # Adjust metrics to get average loss and accuracy per batch \n",
    "  test_loss = test_loss / len(dataloader)\n",
    "  test_acc = test_acc / len(dataloader)\n",
    "  return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22916e7b-3218-4ec0-9e4d-2ef2dc7c0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device) -> Dict[str, List[float]]:\n",
    "  \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "  Passes a target PyTorch models through train_step() and test_step()\n",
    "  functions for a number of epochs, training and testing the model\n",
    "  in the same epoch loop.\n",
    "\n",
    "  Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model to be trained and tested.\n",
    "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    epochs: An integer indicating how many epochs to train for.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "  Returns:\n",
    "    A dictionary of training and testing loss as well as training and\n",
    "    testing accuracy metrics. Each metric has a value in a list for \n",
    "    each epoch.\n",
    "    In the form: {train_loss: [...],\n",
    "                  train_acc: [...],\n",
    "                  test_loss: [...],\n",
    "                  test_acc: [...]} \n",
    "    For example if training for epochs=2: \n",
    "                 {train_loss: [2.0616, 1.0537],\n",
    "                  train_acc: [0.3945, 0.3945],\n",
    "                  test_loss: [1.2641, 1.5706],\n",
    "                  test_acc: [0.3400, 0.2973]} \n",
    "  \"\"\"\n",
    "  # Create empty results dictionary\n",
    "  results = {\"train_loss\": [],\n",
    "      \"train_acc\": [],\n",
    "      \"test_loss\": [],\n",
    "      \"test_acc\": []\n",
    "  }\n",
    "  \n",
    "  # Loop through training and testing steps for a number of epochs\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "      train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          device=device)\n",
    "      test_loss, test_acc = test_step(model=model,\n",
    "          dataloader=test_dataloader,\n",
    "          loss_fn=loss_fn,\n",
    "          device=device)\n",
    "      \n",
    "      # Print out what's happening\n",
    "      print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "      )\n",
    "\n",
    "      # Update results dictionary\n",
    "      results[\"train_loss\"].append(train_loss)\n",
    "      results[\"train_acc\"].append(train_acc)\n",
    "      results[\"test_loss\"].append(test_loss)\n",
    "      results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "  # Return the filled results at the end of the epochs\n",
    "  return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "733e705b-9ea7-4c1d-9970-7d8ffc1fb44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str):\n",
    "  \"\"\"Saves a PyTorch model to a target directory.\n",
    "\n",
    "  Args:\n",
    "    model: A target PyTorch model to save.\n",
    "    target_dir: A directory for saving the model to.\n",
    "    model_name: A filename for the saved model. Should include\n",
    "      either \".pth\" or \".pt\" as the file extension.\n",
    "  \n",
    "  Example usage:\n",
    "    save_model(model=model_0,\n",
    "               target_dir=\"models\",\n",
    "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
    "  \"\"\"\n",
    "  # Create target directory\n",
    "  target_dir_path = Path(target_dir)\n",
    "  target_dir_path.mkdir(parents=True,\n",
    "                        exist_ok=True)\n",
    "  \n",
    "  # Create model save path\n",
    "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "  model_save_path = target_dir_path / model_name\n",
    "\n",
    "  # Save the model state_dict()\n",
    "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "  torch.save(obj=model.state_dict(),\n",
    "             f=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb7fbfbb-be1b-429c-9c96-7a0c6d593c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f12c9ed9b09471690589e4f66966ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.1063 | train_acc: 0.3047 | test_loss: 1.0983 | test_acc: 0.3116\n",
      "Epoch: 2 | train_loss: 1.0995 | train_acc: 0.3320 | test_loss: 1.0699 | test_acc: 0.5417\n",
      "Epoch: 3 | train_loss: 1.0863 | train_acc: 0.4922 | test_loss: 1.0800 | test_acc: 0.5227\n",
      "Epoch: 4 | train_loss: 1.0826 | train_acc: 0.4102 | test_loss: 1.0599 | test_acc: 0.5729\n",
      "Epoch: 5 | train_loss: 1.0630 | train_acc: 0.4141 | test_loss: 1.0609 | test_acc: 0.5540\n",
      "[INFO] Total training time: 7.729 seconds\n",
      "[INFO] Saving model to: models/05_going_modular_cell_mode_tinyvgg_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42) \n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Recreate an instance of TinyVGG\n",
    "model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n",
    "                  hidden_units=10, \n",
    "                  output_shape=len(train_data.classes)).to(device)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "# Train model_0 \n",
    "model_0_results = train(model=model_0, \n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn, \n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        device=device)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n",
    "\n",
    "# Save the model\n",
    "save_model(model=model_0,\n",
    "           target_dir=\"models\",\n",
    "           model_name=\"05_going_modular_cell_mode_tinyvgg_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e041fc-7cef-424e-b0dc-e2332226e789",
   "metadata": {},
   "source": [
    "### Convert cells to python files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95be2766-1d53-4bd1-87a2-00f8677ce558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.transforms.transforms.Compose"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ce4fb14-0829-47dc-8a4a-16d8680e04b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting services/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile services/data_setup.py\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "class SetupData():\n",
    "    \"\"\"Creates training and testing dataloaders and datasets.\n",
    "\n",
    "    Keyword arguments:\n",
    "    train_dir -- Path to train directory.\n",
    "    test_dir -- Path to test directory\n",
    "    data_transform -- torchvision transforms to perform on training and testing data.\n",
    "    batch_size -- number of samples per batch in each dataloader.\n",
    "    num_workers  -- an integer for number of worker per dataloader.\n",
    "\n",
    "    Return:\n",
    "    A tuple of train_dataloder, test_dataloader, class_names.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_dir: str,\n",
    "                 test_dir: str,\n",
    "                 batch_size: int = 32,\n",
    "                 num_workers: int = os.cpu_count()) -> None:\n",
    "        self.train_dir = str(train_dir)\n",
    "        self.test_dir = str(test_dir)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def create_transforms(self, train=False, test=False):\n",
    "        \"\"\"create_transforms create transforms for given split\n",
    "\n",
    "        Args:\n",
    "            train (bool, optional): if transform is for training dataset mark as true. Defaults to False.\n",
    "            test (bool, optional): if transform is for testing dataset mark as true. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            data_transform: transform for given split\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            data_transform = transforms.Compose([\n",
    "                transforms.Resize((64, 64)),\n",
    "                transforms.ToTensor()])\n",
    "        elif test:\n",
    "            data_transform = transforms.Compose([\n",
    "                transforms.Resize((64, 64)),\n",
    "                transforms.ToTensor()])\n",
    "\n",
    "        return data_transform\n",
    "\n",
    "    def create_datasets(self):\n",
    "        \"\"\"create_datasets creates datasets for train and test directories.\n",
    "\n",
    "        Returns:\n",
    "            train_data, test_data: tuple of datasets for splits\n",
    "        \"\"\"\n",
    "        train_transform = self.create_transforms(train=True)\n",
    "        test_transform = self.create_transforms(test=True)\n",
    "\n",
    "        train_data = datasets.ImageFolder(root=self.train_dir,\n",
    "                                          transform=train_transform)\n",
    "\n",
    "        test_data = datasets.ImageFolder(root=self.test_dir,\n",
    "                                         transform=test_transform)\n",
    "        return train_data, test_data\n",
    "\n",
    "    def create_dataloaders(self):\n",
    "        \"\"\"create_dataloaders create dataloaders for train and test datasets.\n",
    "\n",
    "        Returns:\n",
    "            train_dataloader, test_dataloader, class_names: tuple for given split as well as the class_names of the dataset.\n",
    "        \"\"\"\n",
    "        train_data, test_data = self.create_datasets()\n",
    "\n",
    "        class_names = train_data.classes\n",
    "\n",
    "        train_dataloader = DataLoader(dataset=train_data,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      num_workers=self.num_workers,\n",
    "                                      shuffle=True,\n",
    "                                      pin_memory=True)\n",
    "\n",
    "        test_dataloader = DataLoader(dataset=test_data,\n",
    "                                     shuffle=False,\n",
    "                                     batch_size=self.batch_size,\n",
    "                                     num_workers=self.num_workers,\n",
    "                                     pin_memory=True)\n",
    "\n",
    "        return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d8724c0-5cd7-490d-abea-83a53b7d4cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/pizza_steak_sushi/train'),\n",
       " PosixPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "203fd5b8-a858-4df3-a6a0-e234b3dabea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7fddf9e27bb0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7fddf9d8bbe0>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from services.data_setup import SetupData\n",
    "\n",
    "setup_data = SetupData(train_dir=train_dir,\n",
    "                       test_dir=test_dir)\n",
    "\n",
    "train_dataloader, test_dataloader, class_names = setup_data.create_dataloaders()\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0f07740-2120-4c08-b988-afe0e7235d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing services/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile services/model.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "  \"\"\"Creates the TinyVGG architecture.\n",
    "\n",
    "  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
    "  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n",
    "  \n",
    "  Args:\n",
    "    input_shape: An integer indicating number of input channels.\n",
    "    hidden_units: An integer indicating number of hidden units between layers.\n",
    "    output_shape: An integer indicating number of output units.\n",
    "  \"\"\"\n",
    "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "      super().__init__()\n",
    "      self.conv_block_1 = nn.Sequential(\n",
    "          nn.Conv2d(in_channels=input_shape, \n",
    "                    out_channels=hidden_units, \n",
    "                    kernel_size=3, # how big is the square that's going over the image?\n",
    "                    stride=1, # default\n",
    "                    padding=0), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=hidden_units, \n",
    "                    out_channels=hidden_units,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2,\n",
    "                        stride=2) # default stride value is same as kernel_size\n",
    "      )\n",
    "      self.conv_block_2 = nn.Sequential(\n",
    "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(2)\n",
    "      )\n",
    "      self.classifier = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          # Where did this in_features shape come from? \n",
    "          # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "          nn.Linear(in_features=hidden_units*13*13,\n",
    "                    out_features=output_shape)\n",
    "      )\n",
    "    \n",
    "  def forward(self, x: torch.Tensor):\n",
    "      x = self.conv_block_1(x)\n",
    "      x = self.conv_block_2(x)\n",
    "      x = self.classifier(x)\n",
    "      return x\n",
    "      # return self.classifier(self.block_2(self.block_1(x))) # <- leverage the benefits of operator fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97aa9f26-0cdf-473a-a3db-45338b73eb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting services/model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile services/model_builder.py\n",
    "\n",
    "from services.model import TinyVGG\n",
    "\n",
    "\n",
    "class BuildModel():\n",
    "    \"\"\"BuildModel Builds model based on model class in model.py\n",
    "\n",
    "    Arguments:\n",
    "    device -- device where the model will we allocated (cuda-cpu)\n",
    "    input_shape -- number of channels for image (3, 1)\n",
    "    output_shape -- number of classes\n",
    "    hidden_units -- number of hidden units per layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device: str,\n",
    "                 input_shape: int,\n",
    "                 output_shape,\n",
    "                 hidden_units: int):\n",
    "        self.device = device\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"build_model builds model.\n",
    "\n",
    "        Returns:\n",
    "            model: returns model with given parameters\n",
    "        \"\"\"\n",
    "        model = TinyVGG(input_shape=self.input_shape,\n",
    "                        output_shape=self.output_shape,\n",
    "                        hidden_units=self.hidden_units).to(self.device)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f22151d-38c8-4160-b833-2ae6e330f736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = 3\n",
    "input_shape, output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6d7b138-f9d6-4a07-ab01-4587e6f42729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyVGG(\n",
       "  (conv_block_1): Sequential(\n",
       "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1690, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from services.model_builder import BuildModel\n",
    "\n",
    "build_model = BuildModel(device=device,\n",
    "                   input_shape=input_shape,\n",
    "                   output_shape=output_shape,\n",
    "                   hidden_units=10)\n",
    "\n",
    "model = build_model.build_model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82bee6b6-10f5-45df-8bbf-4c392904557f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d26c9729-bc53-4404-bfeb-9865bd3af36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0431, 0.2078, 0.3725,  ..., 0.0824, 0.1333, 0.1020],\n",
       "           [0.2039, 0.2000, 0.2627,  ..., 0.0784, 0.1255, 0.1137],\n",
       "           [0.0980, 0.1176, 0.1333,  ..., 0.1059, 0.1255, 0.1255],\n",
       "           ...,\n",
       "           [0.0196, 0.0196, 0.0235,  ..., 0.0902, 0.1059, 0.0980],\n",
       "           [0.0196, 0.0235, 0.0235,  ..., 0.0863, 0.1020, 0.0941],\n",
       "           [0.0196, 0.0196, 0.0235,  ..., 0.0980, 0.0980, 0.0863]],\n",
       " \n",
       "          [[0.0196, 0.1255, 0.2471,  ..., 0.0235, 0.0353, 0.0235],\n",
       "           [0.1333, 0.1216, 0.1608,  ..., 0.0157, 0.0275, 0.0235],\n",
       "           [0.0588, 0.0667, 0.0784,  ..., 0.0314, 0.0235, 0.0235],\n",
       "           ...,\n",
       "           [0.0039, 0.0039, 0.0078,  ..., 0.0196, 0.0235, 0.0196],\n",
       "           [0.0039, 0.0078, 0.0078,  ..., 0.0196, 0.0235, 0.0196],\n",
       "           [0.0039, 0.0039, 0.0078,  ..., 0.0314, 0.0235, 0.0196]],\n",
       " \n",
       "          [[0.0118, 0.0392, 0.0588,  ..., 0.0039, 0.0078, 0.0039],\n",
       "           [0.0471, 0.0314, 0.0392,  ..., 0.0039, 0.0078, 0.0039],\n",
       "           [0.0314, 0.0392, 0.0392,  ..., 0.0196, 0.0078, 0.0039],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0039, 0.0039],\n",
       "           [0.0000, 0.0039, 0.0039,  ..., 0.0000, 0.0039, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000]]]],\n",
       "        device='cuda:0'),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_batch, label_batch = next(iter(test_dataloader))\n",
    "img, lbl = img_batch[0].unsqueeze(0).to(device), label_batch[0]\n",
    "\n",
    "img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cea2b1-ed45-40f6-90bf-69c3a5af382d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0066, 0.0172, 0.0194]], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    res_test = model(img)\n",
    "\n",
    "res_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59844370-8a80-4a20-a00a-ed99fd06627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = torch.softmax(res_test, dim=1)\n",
    "pred_lbl = torch.argmax(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e35a16f-d0d4-4f2c-8260-89a8225647dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sushi'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[pred_lbl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287aa7cc-19d2-43d5-b4bb-95688ec72665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pizza'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[lbl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8480f7e2-4ece-4256-bea5-aeab8454325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting services/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile services/engine.py\n",
    "\n",
    "from typing import Tuple, Dict, List\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "class TrainTestStep():\n",
    "    \"\"\"\n",
    "  Args:\n",
    "    model: A PyTorch model to be trained and tested.\n",
    "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    epochs: An integer indicating how many epochs to train for.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "  \"\"\"\n",
    "    def __init__(self,\n",
    "                model: torch.nn.Module, \n",
    "                train_dataloader: torch.utils.data.DataLoader, \n",
    "                test_dataloader: torch.utils.data.DataLoader, \n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                loss_fn: torch.nn.Module,\n",
    "                epochs: int,\n",
    "                device: torch.device):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "    \n",
    "    def train_step(self) -> Tuple[float, float]:\n",
    "        \"\"\"Trains a PyTorch model for a single epoch.\n",
    "\n",
    "        Turns a target PyTorch model to training mode and then\n",
    "        runs through all of the required training steps (forward\n",
    "        pass, loss calculation, optimizer step).\n",
    "\n",
    "        Returns:\n",
    "          A tuple of training loss and training accuracy metrics. \n",
    "          In the form (train_loss, train_accuracy). For example:\n",
    "\n",
    "          (0.1112, 0.8743)\n",
    "      \"\"\"\n",
    "        self.model.train()\n",
    "        train_loss, train_acc = 0\n",
    "        for batch, (X, y) in enumerate(self.train_dataloader):\n",
    "            y_pred = self.model(X)\n",
    "            loss = self.loss_fn(y_pred, y)\n",
    "            train_loss += loss.item()\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "            train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "            \n",
    "        train_loss /= len(self.train_dataloader)\n",
    "        train_acc /= len(self.train_dataloader)\n",
    "        \n",
    "        return train_loss, train_acc\n",
    "    \n",
    "    def test_step(self):\n",
    "        \"\"\"Tests a PyTorch model for a single epoch.\n",
    "\n",
    "        Turns a target PyTorch model to \"eval\" mode and then performs\n",
    "        a forward pass on a testing dataset.\n",
    "\n",
    "        Returns:\n",
    "         A tuple of testing loss and testing accuracy metrics.\n",
    "         In the form (test_loss, test_accuracy). For example:\n",
    "\n",
    "          (0.0223, 0.8985)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        test_loss, test_acc = 0\n",
    "        with torch.inference_mode():\n",
    "            for batch, (X_test, y_test) in enumerate(self.test_dataloader):\n",
    "                y_test_logits = self.model(X_test)\n",
    "                y_test_loss = self.loss_fn(y_test_logits, y_test)\n",
    "                test_loss += y_test_loss.item()\n",
    "                self.optimizer.zero_grad()\n",
    "                y_test_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                test_pred_labels = y_test_logits.argmax(dim=1)\n",
    "                test_acc += ((test_pred_labels == y_test).sum().item()/len(test_pred_labels))\n",
    "                \n",
    "            test_loss /= len(self.test_dataloader)\n",
    "            test_acc /= len(self.test_dataloader)\n",
    "            \n",
    "            return test_loss, test_acc\n",
    "        \n",
    "    def train_model(self):\n",
    "        results = {\n",
    "                \"train_loss\": [],\n",
    "                \"train_acc\": [],\n",
    "                \"test_loss\": [],\n",
    "                \"test_acc\": []}\n",
    "            \n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            train_loss, train_acc = self.train_step()\n",
    "            test_loss, test_acc = self.test_step()\n",
    "                \n",
    "            print(f\"Epoch: {epoch+1} | \"\n",
    "                  f\"train_loss: {train_loss:.4f} | \"\n",
    "                  f\"train_acc: {train_acc:.4f} | \"\n",
    "                  f\"test_loss: {test_loss:.4f} | \"\n",
    "                  f\"test_acc: {test_acc:.4f}\")\n",
    "                \n",
    "            results[\"train_loss\"].append(train_loss)\n",
    "            results[\"train_acc\"].append(train_acc)\n",
    "            results[\"test_loss\"].append(test_loss)\n",
    "            results[\"test_acc\"].append(test_acc)\n",
    "                \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c4dd2dbe-4bbd-451d-8bc0-25d8190fc3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CrossEntropyLoss(),\n",
       " Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     lr: 0.001\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e0ad0f47-bec9-4877-bad1-ae9c66583fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.engine import TrainTestStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3536cd84-7cad-4dba-8643-b70c92d87ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_step = TrainTestStep(model=model_0,\n",
    "                      train_dataloader=train_dataloader,\n",
    "                      test_dataloader=test_dataloader,\n",
    "                      optimizer=optimizer,\n",
    "                      loss_fn=loss_fn,\n",
    "                      epochs=5,\n",
    "                      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "db97ace3-5998-4ed7-a0cf-47a5ececfc25",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TrainTestStep' object has no attribute 'train_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [87]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainTestStep' object has no attribute 'train_model'"
     ]
    }
   ],
   "source": [
    "results = train_test_step.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "41852deb-84ac-4fd8-a191-7e43929bc36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting services/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile services/utils.py\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str):\n",
    "    \"\"\"Saves a PyTorch model to a target directory.\n",
    "\n",
    "  Args:\n",
    "    model: A target PyTorch model to save.\n",
    "    target_dir: A directory for saving the model to.\n",
    "    model_name: A filename for the saved model. Should include\n",
    "      either \".pth\" or \".pt\" as the file extension.\n",
    "  \n",
    "  Example usage:\n",
    "    save_model(model=model_0,\n",
    "               target_dir=\"models\",\n",
    "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
    "  \"\"\"\n",
    "  # Create target directory\n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True,\n",
    "                        exist_ok=True)\n",
    "  \n",
    "  # Create model save path\n",
    "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "    model_save_path = target_dir_path / model_name\n",
    "\n",
    "  # Save the model state_dict()\n",
    "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "    torch.save(obj=model.state_dict(),\n",
    "             f=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6da15580-d854-441a-a1e5-06d84fbee2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving model to: models/test.pth\n"
     ]
    }
   ],
   "source": [
    "from services.utils import save_model\n",
    "save_model(model_0, target_dir='models', model_name='test.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8f2a0292-a195-49ee-adfe-7d6f2dc1e5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/pizza_steak_sushi/train'),\n",
       " PosixPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path('./data/pizza_steak_sushi/')\n",
    "train_dir = data_path / 'train'\n",
    "test_dir = data_path / 'test'\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d3c3f836-9e3b-452a-9411-207e08c4a7b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [99]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimeit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_timer \u001b[38;5;28;01mas\u001b[39;00m timer\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel_services\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_builder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BuildModel\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel_services\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_setup\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SetupData\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel_services\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_model\n",
      "File \u001b[0;32m~/projects/pytorch/My notebooks/Pytorch modular/model_services/model_builder.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TinyVGG\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBuildModel\u001b[39;00m():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124;03m\"\"\"BuildModel Builds model based on model class in model.py\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    hidden_units -- number of hidden units per layer\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "# %%writefile services/train.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timeit import default_timer as timer\n",
    "from model_services.model_builder import BuildModel\n",
    "from model_services.data_setup import SetupData\n",
    "from model_services.utils import save_model\n",
    "from model_services.engine import TrainTestStep\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path('./data/pizza_steak_sushi/')\n",
    "save_model_path = Path('./models/')\n",
    "train_dir = data_path / 'train'\n",
    "test_dir = data_path / 'test'\n",
    "save_model_dir = save_model_path\n",
    "model_name = 'test_model_v1.pth'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "setup_data = SetupData(train_dir=train_dir, test_dir=test_dir)\n",
    "train_dataloader, test_dataloader, class_names = setup_data.create_dataloaders()\n",
    "\n",
    "input_shape = 3\n",
    "output_shape = len(class_names)\n",
    "hidden_units = 10\n",
    "\n",
    "build_model = BuildModel(device=device,\n",
    "                         input_shape=input_shape,\n",
    "                         output_shape=output_shape,\n",
    "                         hidden_units=hidden_units)\n",
    "\n",
    "model = build_model.build_model()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "train_test_step = TrainTestStep(model=model,\n",
    "                                train_dataloader=train_dataloader,\n",
    "                                test_dataloader=test_dataloader,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_fn=loss_fn,\n",
    "                                epochs=epochs,\n",
    "                                device=device)\n",
    "\n",
    "results = train_test_step.train_model()\n",
    "\n",
    "save_model(model=model,\n",
    "           target_dir=save_model_path,\n",
    "           model_name=model_name)\n",
    "\n",
    "print(f'Model results : {results}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9962a260-f23f-459c-892b-b3b80931f3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dev/projects/pytorch/My notebooks/Pytorch modular/model_services/train.py\", line 60, in <module>\n",
      "    run()\n",
      "  File \"/home/dev/projects/pytorch/My notebooks/Pytorch modular/model_services/train.py\", line 50, in run\n",
      "    results = train_test_step.train_model()\n",
      "  File \"/home/dev/projects/pytorch/My notebooks/Pytorch modular/model_services/engine.py\", line 104, in train_model\n",
      "    train_loss, train_acc = self.train_step()\n",
      "  File \"/home/dev/projects/pytorch/My notebooks/Pytorch modular/model_services/engine.py\", line 49, in train_step\n",
      "    train_loss, train_acc = 0\n",
      "TypeError: cannot unpack non-iterable int object\n"
     ]
    }
   ],
   "source": [
    "!python model_services/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c2d1c-ae7c-49dc-8513-ba2319ec37af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
